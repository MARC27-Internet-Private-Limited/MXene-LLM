{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/ig3MEMVLKIfGdnntybEH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a121bd6b4999472db4ed921e6a9a3341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20295142900b49b488f0dcf31926a2aa",
              "IPY_MODEL_20c113b131694d6681338b508ed3d816",
              "IPY_MODEL_01451fb146de43278898e2d4cd560975"
            ],
            "layout": "IPY_MODEL_c87b3776dd814ac787e082baf2995c3b"
          }
        },
        "20295142900b49b488f0dcf31926a2aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29d310ba2aee43829df5a0762059f80c",
            "placeholder": "​",
            "style": "IPY_MODEL_46b7e3c2be534390bc206fc848b9e794",
            "value": "Retrieving SummaryDoc documents: 100%"
          }
        },
        "20c113b131694d6681338b508ed3d816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7585bc18d9434abe8ef861942bab2d1c",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4fba48db733a41eba13814fbb03ab21d",
            "value": 33
          }
        },
        "01451fb146de43278898e2d4cd560975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5975c21d4f374bb0b626c60e14b2a3c5",
            "placeholder": "​",
            "style": "IPY_MODEL_67ec6df0368446e98d65c7ee4093dd03",
            "value": " 33/33 [00:00&lt;00:00, 1980.20it/s]"
          }
        },
        "c87b3776dd814ac787e082baf2995c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d310ba2aee43829df5a0762059f80c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b7e3c2be534390bc206fc848b9e794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7585bc18d9434abe8ef861942bab2d1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fba48db733a41eba13814fbb03ab21d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5975c21d4f374bb0b626c60e14b2a3c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67ec6df0368446e98d65c7ee4093dd03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MARC27-Internet-Private-Limited/MXene-LLM/blob/main/MXene.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://research.marc27.com/_app/immutable/assets/logo_marc27.B__kGcan.svg\"\n",
        "       alt=\"Our logo\" width=\"50\">\n",
        "  <h1>P.R.I.S.M.</h1>\n",
        "  <h3><u>P</u>latform for <u>R</u>esearch in <u>I</u>ntelligent <u>S</u>ynthesis of <u>M</u>Xenes</h3>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "2HEzHx-kVyRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Welcome to the **P.R.I.S.M.** notebook to our MXene research project. This notebook sets up a data pipeline that:\n",
        "\n",
        "- **Scrapes and Aggregates Research Data:** Automatically fetches relevant academic articles and patents related to MXene synthesis.\n",
        "- **Trains a Small LLM:** Uses supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) to develop a model that learns to identify the best data. Here we're using: ```deepseek-ai/DeepSeek-R1-Distill-Qwen-7B```\n",
        "\n",
        "\n",
        "- **Populates a Structured Database:** Organizes the curated data for downstream use in training advanced AI models for material discovery.\n",
        "\n",
        "Running on ```Google Colab```, this notebook bridges raw research with AI-driven insights in a smarter way.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "j_n7mJ_te0JA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "To keep the platform hardware agnostic, we'll be using our github repo for the project:\n",
        "```https://github.com/MARC27-Internet-Private-Limited/MXene-LLM.git```\n",
        "\n",
        "Please generate a Personal Access Token on Github to access write privileges to the repositories files\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gLX5RrPZw9bP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "collapsed": true,
        "id": "Qi8SmxNz0ttV",
        "outputId": "073dff60-f8a8-4292-c846-c9851960aafc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ed4b7e47c963>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mrepo_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mxene_project\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GITHUB_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter GitHub token: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mauth_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://{token}:x-oauth-basic@github.com/MARC27-Internet-Private-Limited/MXene-LLM.git\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRepo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             )\n\u001b[0;32m-> 1159\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from getpass import getpass\n",
        "\n",
        "# Setup logging\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Install Git if not present\n",
        "if 'kaggle' in sys.modules:\n",
        "    !pip install gitpython --quiet\n",
        "else:\n",
        "    try:\n",
        "        import git\n",
        "    except ImportError:\n",
        "        !pip install gitpython --quiet\n",
        "\n",
        "# Network check\n",
        "def check_network():\n",
        "    try:\n",
        "        subprocess.run([\"ping\", \"-c\", \"4\", \"github.com\"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        logger.info(\"Network check: GitHub reachable\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        logger.error(\"Network check: GitHub not reachable\")\n",
        "        return False\n",
        "\n",
        "# GitHub setup\n",
        "repo_url = \"https://github.com/MARC27-Internet-Private-Limited/MXene-LLM.git\"\n",
        "repo_dir = os.path.join(os.getcwd(), \"mxene_project\")\n",
        "if not os.path.exists(repo_dir):\n",
        "    if 'kaggle' in sys.modules:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "        token = UserSecretsClient().get_secret(\"GITHUB_TOKEN\") or getpass(\"Enter GitHub token: \")\n",
        "    else:\n",
        "        token = os.environ.get('GITHUB_TOKEN') or getpass(\"Enter GitHub token: \")\n",
        "\n",
        "    logger.info(\"GitHub token provided\" if token else \"No GitHub token\")\n",
        "    auth_url = f\"https://{token}:x-oauth-basic@github.com/MARC27-Internet-Private-Limited/MXene-LLM.git\"\n",
        "\n",
        "    if check_network():\n",
        "        from git import Repo\n",
        "        try:\n",
        "            Repo.clone_from(auth_url, repo_dir, progress=None, verbose=True)\n",
        "            logger.info(f\"Cloned repo to {repo_dir}\")\n",
        "        except git.GitCommandError as e:\n",
        "            logger.error(f\"Clone failed: {e}\")\n",
        "            raise\n",
        "    else:\n",
        "        logger.error(\"Skipping clone—network unavailable\")\n",
        "        raise Exception(\"Network failure—retry later\")\n",
        "else:\n",
        "    logger.info(f\"Repo already exists at {repo_dir}\")\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "print(f\"Working in {repo_dir}\")\n",
        "\n",
        "# GPU check\n",
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "packages = \"transformers datasets accelerate trl bitsandbytes pandas peft gradio gitpython tensorboard\"\n",
        "try:\n",
        "    import transformers, datasets, trl, bitsandbytes, pandas, peft, gradio, git, torch\n",
        "except ImportError:\n",
        "    !pip install {packages} --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "a121bd6b4999472db4ed921e6a9a3341",
            "20295142900b49b488f0dcf31926a2aa",
            "20c113b131694d6681338b508ed3d816",
            "01451fb146de43278898e2d4cd560975",
            "c87b3776dd814ac787e082baf2995c3b",
            "29d310ba2aee43829df5a0762059f80c",
            "46b7e3c2be534390bc206fc848b9e794",
            "7585bc18d9434abe8ef861942bab2d1c",
            "4fba48db733a41eba13814fbb03ab21d",
            "5975c21d4f374bb0b626c60e14b2a3c5",
            "67ec6df0368446e98d65c7ee4093dd03"
          ]
        },
        "collapsed": true,
        "id": "oaLr5a_b4h8T",
        "outputId": "5d00026a-538d-4f4a-dc64-bfc9ce026c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping arXiv...\n",
            "arXiv scraped: 30 abstracts\n",
            "Scraping Materials Project...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Retrieving SummaryDoc documents:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a121bd6b4999472db4ed921e6a9a3341"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Materials Project scraped: 30 abstracts\n",
            "Scraping PubMed...\n",
            "PubMed scraped: 22 abstracts\n",
            "Scraped abstracts - MXene/MAX dataset ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer, PPOTrainer, PPOConfig\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from git import Repo\n",
        "import torch\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "ZzMRtl5u8YP1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Loading model...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=\"float16\") if torch.cuda.is_available() else None\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
        "    quantization_config=bnb_config if bnb_config else None,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
        "logger.info(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "id": "LpVsjWTwTxiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Loading abstracts...\")\n",
        "abstracts_df = pd.read_csv('abstracts.csv')\n",
        "logger.info(f\"Loaded {len(abstracts_df)} abstracts from abstracts.csv\")"
      ],
      "metadata": {
        "id": "gaF3HI2PTym5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Preparing initial training data...\")\n",
        "data = [\n",
        "    {\"input\": \"Ti₃AlC₂ synthesized with HF etching, a=3.07Å, c=18.5Å.\", \"output\": '{\"category\": \"properties\", \"composition\": \"Ti3AlC2\", \"lattice_a\": \"3.07Å\", \"lattice_c\": \"18.5Å\", \"etchant\": \"HF\"}'},\n",
        "    {\"input\": \"Cr₂GaN manufacturing via sputtering.\", \"output\": '{\"category\": \"manufacturing\", \"composition\": \"Cr2GaN\"}'},\n",
        "    {\"input\": \"Nb₄C₃Tₓ tested for supercapacitors.\", \"output\": '{\"category\": \"testing\", \"mxene\": \"Nb4C3Tx\"}'},\n",
        "    {\"input\": \"Ti₃C₂Tₓ tested for supercapacitors.\", \"output\": '{\"category\": \"testing\", \"mxene\": \"Ti3C2Tx\"}'},\n",
        "    {\"input\": \"Ti₂AlC - lattice a=3.057Å\", \"output\": '{\"category\": \"properties\", \"composition\": \"Ti2AlC\", \"lattice_a\": \"3.057Å\"}'}\n",
        "]\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('finetunedata.csv', index=False)\n",
        "logger.info(\"Initial training data saved - 5 entries\")"
      ],
      "metadata": {
        "id": "A2OBFEY1zpWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Setting up LoRA adapters...\")\n",
        "lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "logger.info(\"LoRA adapters added\")"
      ],
      "metadata": {
        "id": "_17O23Ndzu7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Starting SFT...\")\n",
        "dataset = load_dataset('csv', data_files='finetunedata.csv')\n",
        "def tokenize(examples):\n",
        "    logger.info(\"Tokenizing batch...\")\n",
        "    inputs = tokenizer(examples[\"input\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    outputs = tokenizer(examples[\"output\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
        "    return inputs\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='model_sft',\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=50,\n",
        "    logging_steps=5,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_dir='runs'\n",
        ")\n",
        "trainer = SFTTrainer(model=model, args=training_args, train_dataset=tokenized_dataset[\"train\"])\n",
        "trainer.train()\n",
        "logger.info(\"Initial SFT complete\")"
      ],
      "metadata": {
        "id": "jGyx9gzDzxUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Saving model and logs...\")\n",
        "model.save_pretrained('model_sft_final')\n",
        "tokenizer.save_pretrained('model_sft_final')\n",
        "\n",
        "repo = Repo('.')\n",
        "repo.git.add(all=True)\n",
        "repo.git.commit(m=\"Saved initial SFT model and TensorBoard logs\")\n",
        "repo.git.push()\n",
        "logger.info(\"Model and logs pushed to GitHub\")"
      ],
      "metadata": {
        "id": "cp-1YQq4zz_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_refine(input_text, user_output):\n",
        "    logger.info(f\"Input received: {input_text}\")\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    logger.info(\"Tokenization complete\")\n",
        "    try:\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            num_return_sequences=1,\n",
        "            timeout=5\n",
        "        )\n",
        "        predicted = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        logger.info(f\"Prediction generated: {predicted}\")\n",
        "    except Exception as e:\n",
        "        predicted = f\"Error: {str(e)}\"\n",
        "        logger.error(f\"Generation failed: {e}\")\n",
        "\n",
        "    if user_output:\n",
        "        logger.info(f\"Adding feedback: {user_output}\")\n",
        "        new_data = pd.DataFrame([{\"input\": input_text, \"output\": user_output}])\n",
        "        existing_data = pd.read_csv('finetunedata.csv')\n",
        "        updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
        "        updated_data.to_csv('finetunedata.csv', index=False)\n",
        "        repo.git.add('finetunedata.csv')\n",
        "        repo.git.commit(m=f\"Added feedback - {len(updated_data)} entries\")\n",
        "        repo.git.push()\n",
        "        logger.info(f\"Feedback saved and pushed: {len(updated_data)} entries\")\n",
        "        return predicted, f\"Feedback added - {len(updated_data)} entries\"\n",
        "    return predicted, \"Prediction only\"\n",
        "\n",
        "logger.info(\"Launching Gradio UI...\")\n",
        "interface = gr.Interface(\n",
        "    fn=process_and_refine,\n",
        "    inputs=[gr.Textbox(lines=2, placeholder=\"Enter abstract (e.g., Ti₃AlC₂ synthesized, a=3.07Å)...\"),\n",
        "            gr.Textbox(lines=2, placeholder=\"Correct output (e.g., {'category': 'properties', ...}) or blank\")],\n",
        "    outputs=[gr.Textbox(label=\"Prediction\"), gr.Textbox(label=\"Status\")],\n",
        "    title=\"MXene/MAX Initial Classifier\",\n",
        "    description=\"Test predictions, add feedback—grows finetunedata.csv in GitHub.\"\n",
        ")\n",
        "interface.launch(share=True, server_name=\"0.0.0.0\" if 'kaggle' not in sys.modules else None)"
      ],
      "metadata": {
        "id": "xKvq8tOMz147"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logger.info(\"Starting RL (stub - run later)...\")\n",
        "# ppo_config = PPOConfig(model_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", learning_rate=1e-5, batch_size=1)\n",
        "# ppo_trainer = PPOTrainer(model=model, config=ppo_config, tokenizer=tokenizer, dataset=tokenized_dataset[\"train\"])\n",
        "#\n",
        "# def reward_func(pred, true):\n",
        "#     return 1.0 if pred == true else -0.5\n",
        "#\n",
        "# for epoch in range(5):\n",
        "#     for batch in tokenized_dataset[\"train\"]:\n",
        "#         inputs = tokenizer(batch[\"input\"], return_tensors=\"pt\").to(device)\n",
        "#         outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "#         pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "#         reward = reward_func(pred, batch[\"output\"])\n",
        "#         ppo_trainer.step([inputs[\"input_ids\"][0]], [outputs[0]], [reward])\n",
        "# logger.info(\"RL stub complete - expand in Notebook 3\")"
      ],
      "metadata": {
        "id": "0DsEupSHz4EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logger.info(\"Merging with MXene-DB (stub - run in Notebook 2)...\")\n",
        "# mxene_db_url = \"https://raw.githubusercontent.com/diegonti/MXene-DB/main/mxene_db.csv\"\n",
        "# mxene_db = pd.read_csv(mxene_db_url)\n",
        "# mxene_subset = mxene_db[['full_name', 'a', 'Eg_PBE']].rename(columns={'full_name': 'composition', 'a': 'lattice_a', 'Eg_PBE': 'bandgap'})\n",
        "# abstracts_df = pd.read_csv('abstracts.csv')\n",
        "# combined_df = pd.concat([abstracts_df, mxene_subset], ignore_index=True)\n",
        "# combined_df.to_csv('combined_data.csv', index=False)\n",
        "# repo.git.add('combined_data.csv')\n",
        "# repo.git.commit(m=\"Merged with MXene-DB\")\n",
        "# repo.git.push()\n",
        "# logger.info(\"MXene-DB merged - stub complete\")"
      ],
      "metadata": {
        "id": "yo6NylDez5I8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}